{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "<br></br>\n",
    "Before we look at the Database Prediction problem and start programming, let's take a look at the theory behind the Artificial Neural Network algorithm which was popularized by Geoffrey Hinton in the 1980's and is used in Deep Machine Learning. \"Deep\" in Deep Learning refers to all the hidden layers used in this type of Dynamic Programming algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The input layer observations and related output refer to ONE row of data. Adjustment of weights is how Neural Nets learn, they decide the strength and importance of signals that are passed along or blocked by an Activation Function. They keep adjusting weights until the predicted output closely matches the actual output.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/01%20-%20Deep%20Learning.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is a zoomed in version of the node diagram. Yellow nodes represent inputs, green nodes are the hidden layers, and red nodes are outputs.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/02%20-%20Neuron.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Feature Scaling (Standardize or Normalize) is applied to input variables makes it easy for Neural Nets to process data by bringing their values close to each other, read 'Efficient Back Propagation.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/02_1%20-%20Standardized%20Equation.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/02_2%20-%20Standardized%20Equation.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Activation Function\n",
    "\n",
    "<br></br>\n",
    "Here is a list of some Neural Network Activation Functions. Read 'Deep sparse rectifier neural networks.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "1. Threshold Function - Rigid binary style function\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/03%20-%20Threshold.png\" width=\"400\">\n",
    "\n",
    "2. Sigmoid Function - Smooth, good for output Layers that predict probability\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/04%20-%20Sigmoid.png\" width=\"400\">\n",
    "\n",
    "3. Rectifier Function - Gradually increases as input Value increases\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/05%20-%20Rectifier.png\" width=\"400\">\n",
    "\n",
    "4. Hyperbolic Tangent Function - Similar to Sigmoid Function but values can go below zero\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/06%20-%20Tanh.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Different layers of a Neural Net can use different Activation Functions.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/07%20-%20NN%20Activation%20Example.png\" width=\"600\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Cost Function\n",
    "\n",
    "<br></br>\n",
    "The Cost Function is a plot of the differences between the target and the network's output, which we try to minimize through weight adjustments (Backpropagation) in epochs (one training cycle on the Training Set). Once input information is fed through the network and a y_hat output estimate is found (Forward-propagation), we take the error and go back through the network and adjust the weights (Backpropagation Algorithm). The most common cost function is the Quadratic (Root Mean Square) cost:\n",
    "\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "Cost = \\frac{(\\hat y - y)^2}{2} = \\frac{(Wighted Estimate - Actual)^2}{2} \n",
    "$$\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Read this [Deep Learning Book](http://neuralnetworksanddeeplearning.com/index.html) and this [List of Cost Functions Uses](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications?).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Batch Gradient Descent\n",
    "\n",
    "<br></br>\n",
    "This is a Cost minimization technique that looks for downhill slopes and works on Convex Cost Functions. The function can have any number of dimensions, but we are only able to visualize up to three dimensions.\n",
    "\n",
    "\n",
    "### 1D Gradient Descent\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/09%20-%20Gradient%20Descent%201D.png\" width=\"600\">\n",
    "\n",
    "\n",
    "### 2D Gradient Descent\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/10%20-%20Gradient%20Descent%202D.png\" width=\"300\">\n",
    "\n",
    "\n",
    "### 3D Gradient Descent\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/11%20-%20Gradient%20Descent%203D.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Reinforcement Learning (Stochastic Gradient Descent)\n",
    "\n",
    "<br></br>\n",
    "This method is faster & more accurate than Batch Gradient Descent.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "In order to avoid the Local Minimum trap, we can take more sporadic steps in random directions to increase the likelihood of finding the Global Minimum. We can achieve this by adjusting weights one row at a time (Stochastic Gradient Descent) instead of all-at-once (Batch Gradient Descent). Read 'Neural Network in 13 lines of Python.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/12%20-%20Local%20Min%20Trap.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "These are the steps for Stochastic Gradient Descent:\n",
    "1. Initialize weights to small numbers close to 0 (but NOT 0)\n",
    "2. Input first row of Observation Data into input layer\n",
    "3. Forward-propagate: Apply weights to inputs to get predicted result 'y_hat'\n",
    "4. Compute Error = 'y_hat' - 'y_actual'\n",
    "5. Back-propagate: Update weights according to the Learning Rate and how much they're responsible for the Error.\n",
    "6. Repeat steps 1-5 after each observation (Reinforcement Learning), or after each batch (Batch Gradient Descent)\n",
    "7. Epoch is the Training Set passing through the Artificial Neural Network, more Epochs yield improved results.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Evaluating the ANN\n",
    "\n",
    "<br></br>\n",
    "Be careful when measuring the accuracy of a model. Bias and Variance can differ every time the model is evaluated. To solve this problem, we can use K-Fold Cross Validation which splits the data into multiple segments and averages overall accuracy.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/13%20-%20Bias-Variance%20Tradeoff.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Images/14%20-%20K-Fold%20Cross%20Validation.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Overfitting\n",
    "\n",
    "<br></br>\n",
    "Overfitting is when your model is over-trained on the Training Set and isn't generalized enough. This reduces performance on Test Set predictions.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Indicators of overfitting:\n",
    "\n",
    "1. Training and Test Accuracies have a large difference\n",
    "2. Observing High Accuracy Variance when applying K-Fold Cross Validation\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Solve overfitting with \"Dropout Regularization\", this randomly disables Neurons through iterations so they don't grow too dependent on each other. This helps the Neural Network learns several independent correlations from the data.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sample Problem - Bank Database Prediction\n",
    "\n",
    "<br></br>\n",
    "Let's test our knowledge of Artificial Neural Networks by solving a real world problem. Take a look at 'Bank_Customer_Data.csv' in the Data folder of this repository. This technique can be applied to any or any customer oriented business data set.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "### Problem Description:\n",
    "\n",
    "A Bank (or any business) is trying to improve customer retention. The Bank engineers have put together a table of data about their customers (Name, Age, Location, Income, etc). They also have data on whether customers left the Bank or stayed with them (last column of data).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The Bank is trying to build a Machine Learning model that predicts the likelihood of a customer leaving before it actually happens so they can work on improving customer satisfaction.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "### Code\n",
    "\n",
    "<br></br>\n",
    "You can run the code online with Google Colab which is web based and doesn't require installations. \n",
    "\n",
    "\n",
    "<br></br>\n",
    "The better alternative is to download the code and run it with 'Spyder' found in the [Anaconda Distribution](https://www.anaconda.com/download/). 'Spyder' is similar to MATLAB, it allows you to step through the code and examine the 'Variable Explorer' to see exactly how the data is parsed and analyzed.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Database-Prediction.git\n",
    "$ cd Machine-Learning-Database-Prediction\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Pip Install libraries in Terminal\n",
    "# Install Theano (U. Montreal NumPy computation that can run on GPU or CPU, when parallel Float Point computation is important)\n",
    "# Install Tensorflow (Google, same as above)\n",
    "# Install Keras (Combines the above 2 libraries)\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/Users/mac/Google Drive/Python & RasPi/Udemy Deep Learning/Deep Learning Code/Volume 1 - Supervised Deep Learning/Part 1 - Artificial Neural Networks (ANN)/Churn_Modelling.csv')\n",
    "# Extract Independat Variables (Matrix of Features / Observations)\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "# Extract Dependant Variables Vector\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical (Dep/Indep) data\n",
    "# We need to convert non-number data into numbers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# In the Bank example: Convert France/Germany/Spain into 0/1/2\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "# In the Bank example: Convert Female/Male into 0/1\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "# Since our country categorical data is not ordinal (order doesn't matter)\n",
    "# We need to create a Dummy variable\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "# Make all Depedent X objects have the same type (Float 64)\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# Remove  column to avoid Dummy Variable trap\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "# In Bank example we dont need to encode Dependent variables because it's already Binary\n",
    "# Uncomment to activate the following code\n",
    "#labelencoder_y = LabelEncoder()\n",
    "#y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Test_size = 0.2 means 80% of data for training, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "# This steps Standardizes Input Data to ease computation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 2 - Making the ANN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "\n",
    "# Required to initialize NN\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Required to build Deep layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Prevent Overfitting with Dropout Regularization\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the ANN Sequentially (can also initialize as Graph)\n",
    "# We use Sequential because we have successive layers\n",
    "# We call our NN \"Classifier\"\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layerx\n",
    "# This step initializes the Wights to small random numbers\n",
    "# 'Units' is the number of hidden layers (begin with average of Input & Output layers = 11+1/2 = 6)\n",
    "# 'Kernel_initializer': Initialize weights as small random numbers\n",
    "# 'Input_dim': number Independent Variables\n",
    "# 'Activation': Rectifier Activation Function ('relu') for Hidden Layers, Sigmoid Function for Output Layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Add Dropout Regularization to first layer to prevent Overfitting\n",
    "# 'p': Fraction of Neurons to drop. Start with 0.1 (10% dropped) and increment by 0.1 until Overfitting is solved, don't go over 0.5\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "\n",
    "# Compiling the ANN\n",
    "# 'optimizer': Algorithm used to find the best Weights. 'adam' is a popular Stochastic Gradient Descent Algorithm\n",
    "# 'loss' = 'binary_crossentropy' is useful for Binary Outputs with logarithmic functions\n",
    "# 'loss' = 'categorical_crossentropy' is useful for 3+ categorical Outputs\n",
    "# 'metrics' =  Used to evaluate the ANN, requires list. We use 1 metric called 'accuracy'  \n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "# Experiment to find best 'batch_size' and 'epochs'\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# This gives a vector of probablities of Customers leaving the bank\n",
    "# You can rank the probabilities of customers most likely to leave the bank\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Choose a threshold of which customers leave or stay (use 50% as a starting threshold)\n",
    "# This line converts probabilities into True/False\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "\n",
    "# Predicting a single new observation\n",
    "# Predict if the customer with the following informations will leave the bank:\n",
    "# Geography: France\n",
    "# Credit Score: 600\n",
    "# Gender: Male\n",
    "# Age: 40\n",
    "# Tenure: 3\n",
    "# Balance: 60000\n",
    "# Number of Products: 2\n",
    "# Has Credit Card: Yes\n",
    "# Is Active Member: Yes\n",
    "# Estimated Salary: 50000\n",
    "# sc.transform Feature Scales the new prediction so the model will understand it\n",
    "# Set 1 element as a float64 to set all to float64\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "# Tells you the number of correct vs. incorrect observations\n",
    "# In the Confusion Matrix we get [1,1] + [2,2] Correct Predictions\n",
    "# In the Confusion Matrix we get [1,2] + [2,1] Incorrect Predictions\n",
    "# Compute accuracy = correct predictions / total predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Measure accuracy percentage of the Training Set\n",
    "accuracy = (cm[0,0] + cm[1,1])/2000*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 4 - Evaluating the ANN\n",
    "\n",
    "# Evaluating the ANN\n",
    "# Import K-Fold Cross Validation Libraries\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# Set up NN as a function\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "# 'estimator': Object used to fit the data\n",
    "# 'X': Features of the Training set\n",
    "# 'y': Target variable of Training set\n",
    "# 'cv': Number of Train Test Folds for K-Fold Cross Validation, start with 10, check for low Bias\n",
    "# 'n_jobs': How many CPU cores to use. Use '-1' to use all available CPU cores for parallel computation\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "# We're looking for low Bias (means high Accuracy) & low Variance\n",
    "# We will get 10 Accuracies\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 5 - Improving and Tuning the ANN\n",
    "\n",
    "# Dropout Regularization to reduce overfitting if needed\n",
    "# GridSearch tries several Tuning Hyper Parameters to find the best ones\n",
    "\n",
    "# Tuning the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Try sklearn.grid_search if sklearn.model_selection doesn't work\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# This function has an input (Optimizer) so we can try different ones\n",
    "# 'Adam' and 'rmsprop' (also good for RNN) are good optimizers for stochastic gradient descent\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "# Build NN Classifier, we will train with K-Fold Cross Validation\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [100, 500],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "# Fit Model to data using grid_search to try various Hyper Parameter\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "# Output best parameters\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
